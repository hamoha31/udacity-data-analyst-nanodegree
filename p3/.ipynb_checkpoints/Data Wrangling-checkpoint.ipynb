{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling Project with Python and MongoDB\n",
    "## 1.What is Data Wrangling?\n",
    "## 2.Project Explanation\n",
    "## 3.Data Exploration\n",
    "## 4.Conclusion\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.What is data wrangling?\n",
    "#### Definition: \n",
    "The process of manually converting or mapping data from one \"raw\" form into another format that allows for more convenient consumption of the data with the help of semi-automated tools.\n",
    "#### Why do we need it?\n",
    "After having the data, usually the data's shape is not consistant and clean. So, we need to reshape the data and clean unnecessary things. Espisally when we take data using web scrapping methods and when combining multiple sources of data together. \n",
    "#### Tools used:\n",
    "To clean (wrangle) data, you can do in one of many ways. The listed tools below are most used in these days.\n",
    "<ol>\n",
    "    <li>Manually by hand [Not good way with huge data].</li>\n",
    "    <li>Programming [Scripts to complete a spesific task, excellent with most cases].</li>\n",
    "    <li>Applications [Commerical or open source applications, most of them works with just a small list of data types].</li>\n",
    "</ol>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.Project Explanation:\n",
    "###### The project aims to help the data analyst understanding the wrangling process, from getting data cleaning it and then store it on local database. \n",
    "This project is about the Open Street Map, which is a collaborative Geographical Information System (GIS). Open Street Map allows the community adding, updating and deleting data from maps. Open Street Map (OSM) lunched by Steve Coast 12 years ago and it has more than 3 milion users. \n",
    "\n",
    "###### How project works?\n",
    "To complate the project you need to download an OSM XML dataset for one of areas you are intrested in. Then audit the data in many ways.\n",
    "<ul>\n",
    "    <li>Languages</li>\n",
    "    <li>Data formats</li>\n",
    "    <li>Abbreviation</li>\n",
    "    <li>Etc...</li>\n",
    "</ul>\n",
    "\n",
    "Also, statistical overview of the dataset must be provided for:\n",
    "<ul>\n",
    "    <li>Size of the file</li>\n",
    "    <li>Number of unique users</li>\n",
    "    <li>Number of nodes and ways</li>\n",
    "    <li>Number of chosen type of nodes, like cafes, shops etc.</li>\n",
    "</ul>\n",
    "\n",
    "Finally, you need to provide suggestions for improving, analyzing the data and includes thoughtful discussion about the benefits as well as some anticipated problems in implementing the improvement.\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Exploration\n",
    "I selected Cape Town, South Africa as my area to be cleaned. I will do the following steps:\n",
    "<ol>\n",
    "    <li>Download map's data</li>\n",
    "    <li>Prepare workspace</li>\n",
    "    <li>Select dataset file</li>\n",
    "    <li>Check file size</li>\n",
    "    <li>Calculate total tags</li>\n",
    "    <li>Calculate total users</li>\n",
    "    <li>Audit abbreviations</li>\n",
    "    <li>Apply changes</li>\n",
    "    <li>Convert data JSON</li>\n",
    "    <li>Storing data in MongoDB</li>\n",
    "    <li>Statistical overview</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Download Map's Data\n",
    "I Downloaded Cape Town from the https://mapzen.com/data/metro-extracts datasets. The dataset path is https://s3.amazonaws.com/metro-extracts.mapzen.com/cape-town_south-africa.osm.bz2\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Prepare Workspace\n",
    "I used **Python** and **MongoDB** to clean and query data. **Python** is one of the best languages for this task. It is very fast and lightweight. In the other side, **MongoDB** is one of the fastest NoSQL databases. I used it to store my dataset after cleaning it. It is very powerful tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### 3.Select Dataset File\n",
    "My dataset is huge, so I have to create a sample to make the process faster. I created a Python script called **_sampler.py_** to make the sample. This allows us to check our code faster. The final result of the script it to make a **_sample.osm_** file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The main file\n",
    "OSM_FILE = 'cape-town_south-africa.osm'\n",
    "# The sample file\n",
    "#OSM_FILE = 'sample.osm'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.Check File Size\n",
    "The main dataset file's size is **283.0 MB** where the sample dataset file's size is **5.7 MB**. The dataset size has to be more than **50.00 MB**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('File size', '5.7 MB')\n"
     ]
    }
   ],
   "source": [
    "# this function takes a number of file size in bytes and convert it to other sizes\n",
    "def convert_bytes_to_size(num):\n",
    "    for x in ['Bytes', 'KB', 'MB', 'GB', 'TB']:\n",
    "        if num < 1024.0:\n",
    "            return \"%3.1f %s\" % (num, x)\n",
    "        num = num / 1024.0\n",
    "\n",
    "def get_file_size(file_path):\n",
    "    if os.path.isfile(file_path):\n",
    "        file_info = os.stat(file_path)\n",
    "        return convert_bytes_to_size(file_info.st_size)\n",
    "    \n",
    "size = get_file_size(OSM_FILE)\n",
    "print ('File size', size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The size of the dataset is 283 MB. Whihe means it is bigger than 50 MB.\n",
    "\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.Calculate Total Tags\n",
    "The dataset is in XML like format, which means it made up from many open and close tags like this < tag >Content</ tag >. Knowing the tags count will help us understanding dataset structure and details. \n",
    "\n",
    "The below table list all tags in the dataset with its count.\n",
    "\n",
    "_To know more about tags you can check OpenStreetMap wiki._\n",
    "\n",
    "\n",
    "|    Tag   |  Count  |\n",
    "|:--------:|:-------:|\n",
    "|  bounds  |    1    |\n",
    "|  member  |  30724  |\n",
    "|    nd    | 1554908 |\n",
    "|   node   | 1350871 |\n",
    "|    osm   |    1    |\n",
    "| relation |   3033  |\n",
    "|    tag   |  604348 |\n",
    "|    way   |  212159 | \n",
    "\n",
    "\n",
    "Tags in the dataset can be one the following:\n",
    "\n",
    "    1- lower: valid tags in lowercase\n",
    "    \n",
    "    2- lower_colon: valid tags with a colon in their names\n",
    "    \n",
    "    3- problemchars: tags with problematic characters\n",
    "    \n",
    "    4- other: other tags that do not fall into the other three categories\n",
    "    \n",
    "\n",
    "\n",
    "|      Tag     |  Count |\n",
    "|:------------:|:------:|\n",
    "|     lower    | 555062 |\n",
    "|  lower_colon |  48113 |\n",
    "| problemchars |    7   |\n",
    "|     other    |  1166  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1,\n",
      " 'member': 30724,\n",
      " 'nd': 1554908,\n",
      " 'node': 1350871,\n",
      " 'osm': 1,\n",
      " 'relation': 3033,\n",
      " 'tag': 604348,\n",
      " 'way': 212159}\n"
     ]
    }
   ],
   "source": [
    "# Useing Element Tree, we will loop over the dataset and count the number of appercence of tags.\n",
    "def get_tags_count(filename):\n",
    "        tags_dictionary = {}\n",
    "        for event, element in ET.iterparse(filename):\n",
    "            if element.tag in tags_dictionary: \n",
    "                tags_dictionary[element.tag] = tags_dictionary[element.tag] + 1\n",
    "            else:\n",
    "                tags_dictionary[element.tag] = 1\n",
    "        return tags_dictionary\n",
    "\n",
    "pprint.pprint(get_tags_count(OSM_FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset set contains the above numbers of tags. Which means it is little bit big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 555062, 'lower_colon': 48113, 'other': 1166, 'problemchars': 7}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    reguler expressions where we will check all tags...\n",
    "    \"lower\", valid tags in lowercase,\n",
    "    \"lower_colon\", valid tags with a colon in their names,\n",
    "    \"problemchars\", tags with problematic characters, and\n",
    "    \"other\",other tags that do not fall into the other three categories.\n",
    "'''\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "\n",
    "def key_type(element, keys):\n",
    "    if element.tag == \"tag\":\n",
    "        for tag in element.iter('tag'):\n",
    "            k = tag.get('k')\n",
    "            if lower.search(k):\n",
    "                keys['lower'] += 1\n",
    "            elif lower_colon.search(k):\n",
    "                keys['lower_colon'] += 1\n",
    "            elif problemchars.search(k):\n",
    "                keys['problemchars'] += 1\n",
    "            else:\n",
    "                keys['other'] += 1\n",
    "    return keys\n",
    "\n",
    "\n",
    "def check_key_types(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for event, element in ET.iterparse(filename):\n",
    "        keys = key_type(element, keys)\n",
    "\n",
    "    return keys\n",
    "\n",
    "keys = check_key_types(OSM_FILE)\n",
    "pprint.pprint(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of dataset tags are valid with lowercase style. Where 7 tags are not valid.\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.Calculate Total Users\n",
    "The number of users who helped in editing the map is very good indicator to know the comunity. A bigger number means more comunity, where a smaller number means usually using automated bots.\n",
    "\n",
    "Our dataset contains **1538 users** which is a good number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1538"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check all elemetn and who edit them. We are using set to count users only once [not allowing duplication].\n",
    "def get_users_set(filename):\n",
    "    users_set = set()\n",
    "    for event, element in ET.iterparse(filename):\n",
    "        for item in element:\n",
    "            if 'uid' in item.attrib:\n",
    "                users_set.add(item.attrib['uid'])\n",
    "    return users_set\n",
    "users = get_users_set(OSM_FILE)\n",
    "len(users)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset puplated and edited by 1538 users.\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.Audit Abbreviations\n",
    "Here we will list all appriviations and match them with current appriviations in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A reguler expresion to check the last word in element name which is usally its type [street, road, etc...] \n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "# What we want to see\n",
    "expected = [\"Avenue\", \"Boulevard\", \"Commons\", \"Court\", \"Drive\", \"Lane\", \"Parkway\", \n",
    "                         \"Place\", \"Road\", \"Square\", \"Street\", \"Trail\", \"Circle\", \"Highway\"]\n",
    "\n",
    "# Mapping appriviations to expected.\n",
    "mapping = {'Ave'  : 'Avenue',\n",
    "           'Blvd' : 'Boulevard',\n",
    "           'Dr'   : 'Drive',\n",
    "           'Ln'   : 'Lane',\n",
    "           'Pkwy' : 'Parkway',\n",
    "           'Rd'   : 'Road',\n",
    "           'Rd.'   : 'Road',\n",
    "           'St'   : 'Street',\n",
    "           'st'   : 'Street',\n",
    "           'street' :\"Street\",\n",
    "           'stre' :\"Street\",\n",
    "           'stree' :\"Street\",\n",
    "           'Ct'   : \"Court\",\n",
    "           'Cir'  : \"Circle\",\n",
    "           'Cr'   : \"Court\",\n",
    "           'ave'  : 'Avenue',\n",
    "           'Hwg'  : 'Highway',\n",
    "           'Hwy'  : 'Highway',\n",
    "           'Sq'   : \"Square\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Beach Rd. => 1 Beach Road\n",
      "Main Rd => Main Road\n",
      "Foam Rd => Foam Road\n",
      "Solan st => Solan Street\n",
      "Caledon St => Caledon Street\n",
      "De Villiers St => De Villiers Street\n",
      "New Church street => New Church Street\n",
      "Main Rd => Main Road\n",
      "Barrack St => Barrack Street\n",
      "Rhine Road & Eindhoven street => Rhine Road & Eindhoven Street\n",
      "test stree => test Street\n",
      "Main Rd => Main Road\n",
      "Doncaster St => Doncaster Street\n",
      "Prospect Rd => Prospect Road\n",
      "Pienaar Rd => Pienaar Road\n",
      "Bank St => Bank Street\n",
      "Bank St => Bank Street\n",
      "Bank St => Bank Street\n",
      "Bank St => Bank Street\n",
      "tygerberg street => tygerberg Street\n"
     ]
    }
   ],
   "source": [
    "# This function takes a dictunary of types and the street value. \n",
    "# If its ending not in expected add it to the dictonary.\n",
    "def audit_street_type(street_types_dict, value):\n",
    "    m = street_type_re.search(value)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types_dict[street_type].add(value)\n",
    "\n",
    "# Take a value and check if it is not accepted, then change it.             \n",
    "def audit_street(value):\n",
    "    m = street_type_re.search(value)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            try:\n",
    "                new_type = mapping[street_type]\n",
    "                value = value.replace(street_type, new_type)\n",
    "            except:\n",
    "                return value\n",
    "    return value\n",
    "\n",
    "# This function takes a filename, loop over nodes and ways then check if it has an address with street.\n",
    "# If it is, audit its type.\n",
    "def audit_street_names(filename):\n",
    "    dataset = open(filename, \"r\")\n",
    "    street_types_dict = defaultdict(set)\n",
    "    for event, element in ET.iterparse(dataset, events=(\"start\",)):\n",
    "        if element.tag == \"node\" or element.tag == \"way\":\n",
    "            for tag in element.iter(\"tag\"):\n",
    "                if tag.attrib['k'] == \"addr:street\":\n",
    "                    newval = audit_street(tag.attrib['v'])\n",
    "                    if newval != tag.attrib['v']:\n",
    "                        print tag.attrib['v'] + \" => \" + newval \n",
    "\n",
    "    return street_types_dict\n",
    "\n",
    "types = audit_street_names(OSM_FILE)\n",
    "#pprint.pprint(dict(types)['street'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We grouped items based on the endings.\n",
    "The result above for **street** endings. \n",
    "\n",
    "<hr />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.Apply changes\n",
    "##### 8.1 Change Appriviations\n",
    "Now, we will change old and wrong appriviations to the expected ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This function takes the old name, the mapping of appriviations, and the types reguler expresion\n",
    "# It try to find a regex in the old name\n",
    "# If found, it update it then return the new name\n",
    "# If not found, return old name as it is.\n",
    "def update_name(name, mapping, regex):\n",
    "    m = regex.search(name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type in mapping:\n",
    "            name = re.sub(regex, mapping[street_type], name)\n",
    "\n",
    "    return name\n",
    "\n",
    "# For each type in types dictionary. Print the old name and the updated name. \n",
    "# Ex. Foam Rd => Foam Road\n",
    "for street_type, ways in types.iteritems():\n",
    "    for name in ways:\n",
    "        better_name = update_name(name, mapping, street_type_re)\n",
    "        if len(better_name) != len(name):\n",
    "            print name, \"=>\", better_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above are sample from the changed items after procceing them.\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 8.2 Change Cities Names\n",
    "As we selected Cape Town, some small cities were selected with it. So, we are going to clean how cities writen. The wat cities have to be writen is Capitel way. Which means cape Town, cape town, Cape town or cape-town are not accepted. It has to be Cape Town. \n",
    "\n",
    "I went over the data and made a set with cities names. Where I can make a list of all used names. Then I changed the wrong names with better ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Names in the dataset.\n",
    "expected_city_names = ['Abbotsdale', 'Athlone', 'Atlantis', 'Belgravia', 'Bellville',\n",
    "     'Bergvliet', 'Blouberg', 'Blue Downs', 'Bothasig', 'Brackenfell', 'Camps Bay', 'Cape Town',\n",
    "     'Capetown', 'Capricorn', 'Century City', 'Claremont', 'Constantia', 'De Tijger',\n",
    "     'Delft Cape Town', 'Diep River', 'Durbanville', 'Epping', 'Filippi',\n",
    "     'Foreshore, Cape Town', 'Gardens', 'Glencairn Heights', 'Goodwood', 'Gordons Bay',\n",
    "     'Grassy Park', 'Green Point', 'Hout Bay', 'Hout Bay Harbour', 'Hout Bay Heights Estate',\n",
    "     'Kalbaskraal', 'Kapstaden', 'Kenilworth', 'Khayelitsha', 'Killarney Gardens, Cape Town',\n",
    "     'Kommetjie', 'Kuilsriver', 'Kuilsrivier', 'Lansdowne', 'Loevenstein','Maitland',\n",
    "     'Makahza', 'Manenberg', 'Marina Da Gama', 'Melkbosstrand', 'Mfuleni',\n",
    "     'Milnerton', 'Milnerton,  Cape Town', 'Mitchells Plain', 'Mowbray', 'Mowbray, Cape Town',\n",
    "     'Muizenberg', 'Nerina Lane', 'Newlands', 'Noordhoek', 'Noordhoek, Cape Town', 'Nyanga',\n",
    "     'Observatory', 'Paarden Eiland' ,'Paarl' ,'Parklands' ,'Parow','Philadelphia',\n",
    "     'Pinelands','Plumstead','Pniel','Pringle Bay','Richwood','Rondebosch','Rondebosch East','Rondebosh East',\n",
    "     'Rosebank','Salt River','Scarborough','Sea Point','Sea Point, Cape Town',\"Simon's Town\",\n",
    "     'Somerset West','Sonnekuil','Steenberg','Stellenbosch','Stellenbosch Farms','Strand',\n",
    "     'Strandfotein','Suider Paarl','Sybrand Park','Table View','Techno Park','Technopark',\n",
    "     'Test city','Vredehoek','Vrygrond','Welgelegen 2','Welgemoed','Wellington','Woodstock',\n",
    "     'Woodstock, Cape Town','Wynberg','Zonnebloem']\n",
    "\n",
    "# Wrong names with better names.\n",
    "city_names_mapping = {\n",
    "         'cape Town' : \"Cape Town\",\n",
    "         'cape town' : \"Cape Town\",\n",
    "         'cape-town' : \"Cape Town\",\n",
    "         'Cape town' : \"Cape Town\",\n",
    "         'muizenberg' : \"Muizenberg\",\n",
    "         'rylands' : \"Rylands\"\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This function takes a value of city name. \n",
    "# If the city name is in expected names, return it. Else, change it with better one.\n",
    "def audit_city(value):\n",
    "    if value not in expected_city_names:\n",
    "        return city_names_mapping[value]\n",
    "    return value\n",
    "\n",
    "# This function takes a filename, loop over nodes and ways then check if it has an address with city name.\n",
    "# If it is, audit the name and change it.\n",
    "def audit_city_name(filename):\n",
    "    dataset = open(filename, \"r\")\n",
    "    cities_list = []\n",
    "    for event, element in ET.iterparse(dataset, events=(\"start\",)):\n",
    "        if element.tag == \"node\" or element.tag == \"way\":\n",
    "            for tag in element.iter(\"tag\"):\n",
    "                if tag.attrib['k'] == \"addr:city\":\n",
    "                    new_tag = tag\n",
    "                    new_name = audit_city(tag.attrib['v'])\n",
    "                    if new_name != tag.attrib['v']:\n",
    "                        print tag.attrib['v'] + \" => \" + new_name\n",
    "                    new_tag.attrib['v'] = new_name\n",
    "                    cities_list.append(new_tag)\n",
    "    return cities_list\n",
    "\n",
    "cities = audit_city_name(OSM_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result above is a sample of changed city names. You can see the wrong names changed with better names. Writing Cape Town as cape town is not accepted. So, we went throw all items and changed them. This is the scound cleaning in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.Convert Data to JSON\n",
    "Our dataset is in XML format. We are going to store data in MonogoDB, but before that we have to change dataset format from XML to JSON. \n",
    "\n",
    "To start doing that, we have to set the structure of the object, then start changing items based on it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "address_regex = re.compile(r'^addr\\:')\n",
    "street_regex = re.compile(r'^street')\n",
    "\n",
    "CREATED = [ \"version\", \"changeset\", \"timestamp\", \"user\", \"uid\"]\n",
    "\n",
    "'''\n",
    "This method takes an element and check if it is a node or a way. If not skip it.\n",
    "It map the XML element to an object where we can store it as JSON.\n",
    "'''\n",
    "def convert_element(element):\n",
    "    node = {}\n",
    "    if element.tag == \"node\" or element.tag == \"way\" :\n",
    "        node['type'] = element.tag\n",
    "        # address details of element\n",
    "        address = {}\n",
    "        \n",
    "        # for each attribute in the element, parse it into object's variable\n",
    "        for attribute in element.attrib:\n",
    "            if attribute in CREATED:\n",
    "                if 'created' not in node:\n",
    "                    node['created'] = {}\n",
    "                node['created'][attribute] = element.get(attribute)\n",
    "            elif attribute in ['lat', 'lon']:\n",
    "                continue\n",
    "            else:\n",
    "                node[attribute] = element.get(attribute)\n",
    "                \n",
    "        # store posstion cordinates if the element has lon and lat [GPS details]\n",
    "        if 'lat' in element.attrib and 'lon' in element.attrib:\n",
    "            node['pos'] = [float(element.get('lat')), float(element.get('lon'))]\n",
    "\n",
    "        # for each sub element of the root one\n",
    "        for sub in element:\n",
    "            # parse second-level tags for ways and populate `node_refs`\n",
    "            if sub.tag == 'nd':\n",
    "                if 'node_refs' not in node:\n",
    "                    node['node_refs'] = []\n",
    "                if 'ref' in sub.attrib:\n",
    "                    node['node_refs'].append(sub.get('ref'))\n",
    "\n",
    "            # skip the sub if it has no k or v\n",
    "            if sub.tag != 'tag' or 'k' not in sub.attrib or 'v' not in sub.attrib:\n",
    "                continue\n",
    "                \n",
    "            key = sub.get('k')\n",
    "            val = sub.get('v')\n",
    "\n",
    "            # skip the key if it is not well prepared\n",
    "            if problemchars.search(key):\n",
    "                continue\n",
    "\n",
    "            # if it is an address, store it in clean way\n",
    "            elif address_regex.search(key):\n",
    "                key = key.replace('addr:', '')\n",
    "                address[key] = val\n",
    "\n",
    "            # for others\n",
    "            else:\n",
    "                node[key] = val\n",
    "                \n",
    "        # clean address and store it in the node\n",
    "        if len(address) > 0:\n",
    "            node['address'] = {}\n",
    "            street_full = None\n",
    "            street_dict = {}\n",
    "            street_format = ['prefix', 'name', 'type']\n",
    "            # for each key in address\n",
    "            for key in address:\n",
    "                val = address[key]\n",
    "                if street_regex.search(key):\n",
    "                    if key == 'street':\n",
    "                        street_full = val\n",
    "                    elif 'street:' in key:\n",
    "                        street_dict[key.replace('street:', '')] = val\n",
    "                else:\n",
    "                    node['address'][key] = val\n",
    "            # assign street_full or fallback to compile street dict\n",
    "            if street_full:\n",
    "                node['address']['street'] = street_full\n",
    "            elif len(street_dict) > 0:\n",
    "                node['address']['street'] = ' '.join([street_dict[key] for key in street_format])\n",
    "        return node\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "\n",
    "def convert_file(filename):\n",
    "    output = \"{0}.json\".format(filename)\n",
    "    data = []\n",
    "    with codecs.open(output, \"w\") as fw:\n",
    "        for event, element in ET.iterparse(filename):\n",
    "            obj = convert_element(element)\n",
    "            if obj:\n",
    "                data.append(obj)\n",
    "                fw.write(json.dumps(obj) + \"\\n\")\n",
    "    return data\n",
    "\n",
    "data_objects = convert_file(OSM_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample of result\n",
    "data_objects[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the dataset converted to JSON format and saved on the local folder.\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.Store Data in MongoDB\n",
    "After cleaning data and store it in a clean structure, we are going to save it on local database using MongoDB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# start connection\n",
    "client = MongoClient('localhost:27017')\n",
    "db = client['map']\n",
    "# add items one by one to DB\n",
    "db.map.drop()\n",
    "for item in data_objects:\n",
    "    db.map.insert_one(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A long process ends with adding all items to the database.\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.Statistical Overview\n",
    "The last step is about showing statistical overview about the database.\n",
    "<ul>\n",
    "    <li>Size of the file</li>\n",
    "    <li>Total elements</li>\n",
    "    <li>Number of unique users</li>\n",
    "    <li>Number of nodes and ways</li>\n",
    "    <li>Number of chosen type of nodes, like cafes, shops etc.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Size of the file\n",
    "What do you think, which file is bigger, the old or the new one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the old file bytes and convert it to size\n",
    "old_size = convert_bytes_to_size(os.path.getsize(OSM_FILE))\n",
    "\n",
    "# get the new file bytes and convert it to size\n",
    "new_size = convert_bytes_to_size(os.path.getsize(OSM_FILE + \".json\"))\n",
    "\n",
    "\n",
    "print (\"The old file size is: {}.\".format(old_size))\n",
    "print (\"The new file size is: {}.\".format(new_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total elements\n",
    "Here is how we can count the number of elements in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# total elements in DB\n",
    "db.map.find().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of unique users\n",
    "Now we are going to calculate the number of users who participated in the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# total number of users\n",
    "unique_users = len(db.map.distinct('created.user'))\n",
    "print (\"Total users who participated in the map is: {} users.\".format(unique_users))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of nodes and ways\n",
    "We are going to count the number of nodes and ways in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# count items where type = way\n",
    "total_ways = db.map.find({'type':'way'}).count()\n",
    "\n",
    "# count items where type = node\n",
    "total_nodes = db.map.find({'type':'node'}).count()\n",
    "\n",
    "print (\"Total ways in the map is: {} ways.\".format(total_ways))\n",
    "print (\"Total nodes in the map is: {} nodes.\".format(total_nodes))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of chosen type of nodes, like cafes, shops etc.\n",
    "We are going to see togather what are the most popular nodes in selected area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "amenities = db.map.aggregate([{\"$match\":{\"amenity\":{\"$exists\":1}}}, {\"$group\":{\"_id\":\"$amenity\",\n",
    "    \"count\":{\"$sum\":1}}}, {\"$sort\":{\"count\":-1}}, {\"$limit\":10}])\n",
    "\n",
    "pprint.pprint(list(amenities))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 5 religons \n",
    "We are going to investigate the **place_of_worship** to see top religons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "religions = db.map.aggregate([{\"$match\":{\"amenity\":{\"$exists\":1}, \"amenity\":\"place_of_worship\"}},\n",
    "                 {\"$group\":{\"_id\":\"$religion\", \"count\":{\"$sum\":1}}},{\"$sort\":{\"count\":-1}}, {\"$limit\":5}])\n",
    "list(religions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Top 5 food types\n",
    "We are going to check best and top 5 food types [restaurant style]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "places = db.map.aggregate([{\"$match\":{\"amenity\":{\"$exists\":1}, \"amenity\":\"restaurant\"}},\n",
    "                           {\"$group\":{\"_id\":\"$cuisine\", \"count\":{\"$sum\":1}}},\n",
    "                           {\"$sort\":{\"count\":-1}}, {\"$limit\":5}])\n",
    "list(places)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total Nando's\n",
    "Nando's is one of the most famous restaurants in South Africa. Here we are going to count how many one exists in Cape Town. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nandos_count = db.map.find({\"$or\":[ {\"name\": \"Nando's\"}, {\"name\": \"Nandos\"}]}).count()\n",
    "\n",
    "print \"Total Number of Nando's Resuturant:\", nandos_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total number of Restaurants and Cafes\n",
    "For a city, to be a better place for visitors it has to contains as many resturants and cafes as possible. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eating_pleaces_count = db.map.find({\"$or\":[ {\"amenity\": \"cafe\"}, {\"amenity\": \"restaurant\"}]}).count()\n",
    "print \"Number of restaurants and cafes:\", eating_pleaces_count\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.Conclusion\n",
    "In conclusion, Open Street Map is an amazing project. It is going to help the community and improve many applications as it is an open source project. I liked the way how people participate in the project and help each other.\n",
    "\n",
    "I suggest the site managers help people developing automated bots to correct the data via Google Maps API. This will allow us to have a copy from Google Map data, also Google Maps will get benefits from the community. Using Google Map with OpenStreetMap via Google Places API for example will help OpenStreetMap improve places details.\n",
    "\n",
    "**Python Google Places** is a library to work with the API. You can find it here:\n",
    "\n",
    "https://github.com/slimkrazy/python-google-places\n",
    "\n",
    "This will allows us to search for location and fill empty details from it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is a smaple and imagination how we can change places details using API.\n",
    "'''\n",
    "for place in places:\n",
    "    if place.phone == None:\n",
    "        place.phone = API.find(place.geolocation).phone\n",
    "    if place.logo == None:\n",
    "        place.logo = API.find(place.geolocation).logo\n",
    "    ...\n",
    "        ...\n",
    "'''\n",
    "print \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This soultion is very well, but we can not be 100% sure that data is correct. Maybe the old data is better than the new, so we are going to fill only empty attributes. \n",
    "\n",
    "\n",
    "Another improvment can be done by making competitions to improve data either manually or automated. This may be a wrong solution or idea becouse many people may add spam content to win.\n",
    "\n",
    "\n",
    "For the datasets structures I think adding a logo attribute to places is a good idea. As well as adding images for the place."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Refs:*\n",
    "<ul>\n",
    "    <li>Wikipedia</li>\n",
    "    <li>Udacity</li>\n",
    "    <li>Stack Overflow</li>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:DAND]",
   "language": "python",
   "name": "conda-env-DAND-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
